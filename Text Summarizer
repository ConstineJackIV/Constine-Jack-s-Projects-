import pandas as pd
import numpy as np
import textwrap
import nltk
from nltk.corpus import stopwords
from nltk import word_tokenize
from nltk.stem import WordNetLemmatizer, PorterStemmer
from sklearn.feature_extraction.text import TfidfVectorizer

nltk.download('punkt')
nltk.download('stopwords')

# https://www.kaggle.com/shivamkushwaha/bbc-full-text-document-classification
!wget -nc https://lazyprogrammer.me/course_files/nlp/bbc_text_cls.csv

df = pd.read_csv('bbc_text_cls.csv')
df.head()

doc = df[df.labels == 'business']['text'].sample(random_state=42)
def wrap(x):
  return textwrap.fill(x, replace_whitespace=False, fix_sentence_endings=True)
print(wrap(doc.iloc[0]))

'''
Christmas sales worst since 1981

UK retail sales fell in December,
failing to meet expectations and making it by some counts the worst
Christmas since 1981.

Retail sales dropped by 1% on the month in
December, after a 0.6% rise in November, the Office for National
Statistics (ONS) said.  The ONS revised the annual 2004 rate of growth
down from the 5.9% estimated in November to 3.2%. A number of
retailers have already reported poor figures for December.  Clothing
retailers and non-specialist stores were the worst hit with only
internet retailers showing any significant growth, according to the
ONS.

The last time retailers endured a tougher Christmas was 23 years
previously, when sales plunged 1.7%.

The ONS echoed an earlier
caution from Bank of England governor Mervyn King not to read too much
into the poor December figures.  Some analysts put a positive gloss on
the figures, pointing out that the non-seasonally-adjusted figures
showed a performance comparable with 2003. The November-December jump
last year was roughly comparable with recent averages, although some
way below the serious booms seen in the 1990s.  And figures for retail
volume outperformed measures of actual spending, an indication that
consumers are looking for bargains, and retailers are cutting their
prices.

However, reports from some High Street retailers highlight
the weakness of the sector.  Morrisons, Woolworths, House of Fraser,
Marks & Spencer and Big Food all said that the festive period was
disappointing.

And a British Retail Consortium survey found that
Christmas 2004 was the worst for 10 years.  Yet, other retailers -
including HMV, Monsoon, Jessops, Body Shop and Tesco - reported that
festive sales were well up on last year.  Investec chief economist
Philip Shaw said he did not expect the poor retail figures to have any
immediate effect on interest rates.  "The retail sales figures are
very weak, but as Bank of England governor Mervyn King indicated last
night, you don't really get an accurate impression of Christmas
trading until about Easter," said Mr Shaw.  "Our view is the Bank of
England will keep its powder dry and wait to see the big picture."
'''

sents = nltk.sent_tokenize(doc.iloc[0].split("\n", 1)[1])
featurizer = TfidfVectorizer(
    stop_words=stopwords.words('english'),
    norm='l1',
)
X = featurizer.fit_transform(sents)
def get_sentence_score(tfidf_row):
  # return the average of the non-zero values
  # of the tf-idf vector representation of a sentence
  x = tfidf_row[tfidf_row != 0]
  return x.mean()
scores = np.zeros(len(sents))
for i in range(len(sents)):
  score = get_sentence_score(X[i,:])
  scores[i] = score
# Exercise: can you think of a way to compute all the scores in
# one step?
sort_idx = np.argsort(-scores)
# Many options for how to choose which sentences to include:

# 1) top N sentences
# 2) top N words or characters.
# 3) top X% sentences or top X% words
# 4) sentences with scores > average score
# 5) sentences with scores > factor * average score

# You also don't have to sort. May make more sense in order.

print("Generated summary:")
for i in sort_idx[:5]:
  print(wrap("%.2f: %s" % (scores[i], sents[i])))

'''
Generated summary:
0.14: A number of retailers have already reported poor figures for
December.
0.13: However, reports from some High Street retailers highlight the
weakness of the sector.
0.12: The ONS revised the annual 2004 rate of growth down from the
5.9% estimated in November to 3.2%.
0.10: "Our view is the Bank of England will keep its powder dry and
wait to see the big picture."
0.10: And a British Retail Consortium survey found that Christmas 2004
was the worst for 10 years.
'''

doc.iloc[0].split("\n", 1)[0]

def summarize(text):
  # extract sentences
  sents = nltk.sent_tokenize(text)

  # perform tf-idf
  X = featurizer.fit_transform(sents)

  # compute scores for each sentence
  scores = np.zeros(len(sents))
  for i in range(len(sents)):
    score = get_sentence_score(X[i,:])
    scores[i] = score
  
  # sort the scores
  sort_idx = np.argsort(-scores)

  # print summary
  for i in sort_idx[:5]:
    print(wrap("%.2f: %s" % (scores[i], sents[i])))
doc = df[df.labels == 'entertainment']['text'].sample(random_state=123)
summarize(doc.iloc[0].split("\n", 1)[1])

doc.iloc[0].split("\n", 1)[0]

print(wrap(doc.iloc[0]))
